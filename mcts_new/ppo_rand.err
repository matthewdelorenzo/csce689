Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:45, 22.56s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:44<00:22, 22.27s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:59<00:00, 18.77s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:59<00:00, 19.75s/it]
Some weights of the model checkpoint at /mnt/shared-scratch/Rajendran_J/matthewdelorenzo/rltf/ppo_codellama13b were not used when initializing LlamaForCausalLM: ['v_head.summary.bias', 'v_head.summary.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
You shouldn't move a model when it is dispatched on multiple devices.
Traceback (most recent call last):
  File "/mnt/shared-scratch/Rajendran_J/matthewdelorenzo/research/mcts_new/main.py", line 125, in <module>
    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
  File "/mnt/shared-scratch/Rajendran_J/matthewdelorenzo/miniconda3/envs/rltf/lib/python3.10/site-packages/accelerate/big_modeling.py", line 426, in wrapper
    return fn(*args, **kwargs)
  File "/mnt/shared-scratch/Rajendran_J/matthewdelorenzo/miniconda3/envs/rltf/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2469, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.11s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.07s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.74s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.83s/it]
Some weights of the model checkpoint at /mnt/shared-scratch/Rajendran_J/matthewdelorenzo/rltf/ppo_codellama13b were not used when initializing LlamaForCausalLM: ['v_head.summary.weight', 'v_head.summary.bias']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
You shouldn't move a model when it is dispatched on multiple devices.
Traceback (most recent call last):
  File "/mnt/shared-scratch/Rajendran_J/matthewdelorenzo/research/mcts_new/main.py", line 125, in <module>
    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
  File "/mnt/shared-scratch/Rajendran_J/matthewdelorenzo/miniconda3/envs/rltf/lib/python3.10/site-packages/accelerate/big_modeling.py", line 426, in wrapper
    return fn(*args, **kwargs)
  File "/mnt/shared-scratch/Rajendran_J/matthewdelorenzo/miniconda3/envs/rltf/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2469, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.10s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.06s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.73s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.82s/it]
Some weights of the model checkpoint at /mnt/shared-scratch/Rajendran_J/matthewdelorenzo/rltf/ppo_codellama13b were not used when initializing LlamaForCausalLM: ['v_head.summary.bias', 'v_head.summary.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
You shouldn't move a model when it is dispatched on multiple devices.
Traceback (most recent call last):
  File "/mnt/shared-scratch/Rajendran_J/matthewdelorenzo/research/mcts_new/main.py", line 125, in <module>
    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
  File "/mnt/shared-scratch/Rajendran_J/matthewdelorenzo/miniconda3/envs/rltf/lib/python3.10/site-packages/accelerate/big_modeling.py", line 426, in wrapper
    return fn(*args, **kwargs)
  File "/mnt/shared-scratch/Rajendran_J/matthewdelorenzo/miniconda3/envs/rltf/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2469, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.08s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.04s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.72s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.81s/it]
Some weights of the model checkpoint at /mnt/shared-scratch/Rajendran_J/matthewdelorenzo/rltf/ppo_codellama13b were not used when initializing LlamaForCausalLM: ['v_head.summary.bias', 'v_head.summary.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
You shouldn't move a model when it is dispatched on multiple devices.
Traceback (most recent call last):
  File "/mnt/shared-scratch/Rajendran_J/matthewdelorenzo/research/mcts_new/main.py", line 125, in <module>
    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
  File "/mnt/shared-scratch/Rajendran_J/matthewdelorenzo/miniconda3/envs/rltf/lib/python3.10/site-packages/accelerate/big_modeling.py", line 426, in wrapper
    return fn(*args, **kwargs)
  File "/mnt/shared-scratch/Rajendran_J/matthewdelorenzo/miniconda3/envs/rltf/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2469, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.13s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.07s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.73s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.83s/it]
Some weights of the model checkpoint at /mnt/shared-scratch/Rajendran_J/matthewdelorenzo/rltf/ppo_codellama13b were not used when initializing LlamaForCausalLM: ['v_head.summary.bias', 'v_head.summary.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
You shouldn't move a model when it is dispatched on multiple devices.
Traceback (most recent call last):
  File "/mnt/shared-scratch/Rajendran_J/matthewdelorenzo/research/mcts_new/main.py", line 125, in <module>
    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
  File "/mnt/shared-scratch/Rajendran_J/matthewdelorenzo/miniconda3/envs/rltf/lib/python3.10/site-packages/accelerate/big_modeling.py", line 426, in wrapper
    return fn(*args, **kwargs)
  File "/mnt/shared-scratch/Rajendran_J/matthewdelorenzo/miniconda3/envs/rltf/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2469, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.15s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.12s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.77s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.87s/it]
Some weights of the model checkpoint at /mnt/shared-scratch/Rajendran_J/matthewdelorenzo/rltf/ppo_codellama13b were not used when initializing LlamaForCausalLM: ['v_head.summary.bias', 'v_head.summary.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
You shouldn't move a model when it is dispatched on multiple devices.
Traceback (most recent call last):
  File "/mnt/shared-scratch/Rajendran_J/matthewdelorenzo/research/mcts_new/main.py", line 125, in <module>
    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
  File "/mnt/shared-scratch/Rajendran_J/matthewdelorenzo/miniconda3/envs/rltf/lib/python3.10/site-packages/accelerate/big_modeling.py", line 426, in wrapper
    return fn(*args, **kwargs)
  File "/mnt/shared-scratch/Rajendran_J/matthewdelorenzo/miniconda3/envs/rltf/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2469, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.
